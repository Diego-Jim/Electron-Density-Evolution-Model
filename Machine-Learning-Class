class ML_Building_and_Training:
    def __init__(self):
        #######################################
        ##### Design aspects of the model #####
        #######################################


        ##### These are the set dimensions the model expects to see #####

        self.spacial_data_size = 99 # so its divisible by 3, drop edgemost point and inner most point
        self.temporal_data_size = 120 # so its divisible by 3
    
    def build_data_sets(self, training_size = 90, Validation_size = 10):
        
        ##################################################
        #### build training data and validation data #####
        ##################################################

        ##### this makes the validation and traiing data sets #####
        MLDP = ML_Data_Preparation(120)
        train_dataset, train_data_labels = MLDP.generate_fake_training_data(training_size) #training data
        validation_dataset, validation_data_labels = MLDP.generate_fake_training_data(validation_size) #validation data

        ##### converts data sets to a format for the NN #####
        train_dataset = tf.data.Dataset.from_tensor_slices((train_dataset, train_data_labels))
        test_dataset = tf.data.Dataset.from_tensor_slices((validation_dataset, validation_data_labels))


        ##### Shuffles training data around  to remove ordering biases #####
        BATCH_SIZE = 5
        SHUFFLE_BUFFER_SIZE = 1

        self.train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
        self.test_dataset = test_dataset.batch(BATCH_SIZE)

        ##### sets the output options for the NN #####
        self.output_options = MLDP.param_options
        self.output_len = len(output_options)
        
    def build_NN(self):
        ##############################
        ##### The Neural Netowrk #####
        ##############################


        # first dense line is the first hidden layer
        # filters is the number of neurons
        #input shape determines the shape of the input, 1 being a vector
        #last dense layer is output layer and number of neurons corresponds to the possible outputs

        self.Seq_NN = Sequential([
                             Conv2D(filters=32, kernel_size=(3), activation='relu', padding='same',input_shape=(temporal_data_size, spacial_data_size,1)),
                             MaxPooling2D(pool_size=(2), strides=2),
                             Conv2D(filters=32, kernel_size=(3), activation='relu', padding='same'),
                             MaxPooling2D(pool_size=(2), strides=2),
                             Conv2D(filters=32, kernel_size=(3), activation='relu', padding='same'),
                             MaxPooling2D(pool_size=(2), strides=2),
                             Flatten(),
                             Dense(units= self.output_len ,activation='softmax'),
        ])

        # Gives a summary of what the model looks like
        self.Seq_NN.summary() 
        
    def train_NN(self):
        ##########################
        ##### Model Training #####
        ##########################

        self.Seq_NN.compile(optimizer='adam',
                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      metrics=['accuracy'])

        history = self.Seq_NN.fit(self.train_dataset, epochs=10, 
                            validation_data=(self.test_dataset))
